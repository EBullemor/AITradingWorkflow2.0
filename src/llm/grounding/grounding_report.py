"""
Grounding Report Generator

Generates comprehensive grounding audit reports for LLM outputs.
"""

from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Dict, List, Optional

from loguru import logger

from .claim_extractor import Claim, ClaimType, ClaimExtractor
from .source_matcher import Source, SourceMatch, GroundingScore, SourceMatcher, GroundingScorer


@dataclass
class GroundingReport:
    """Complete grounding assessment report."""
    # Overall metrics
    overall_score: float  # 0-1, % of claims grounded
    total_claims: int
    grounded_claims: int
    ungrounded_claims: int
    
    # Threshold
    min_threshold: float
    meets_threshold: bool
    
    # Detailed results
    claim_scores: List[GroundingScore] = field(default_factory=list)
    ungrounded_details: List[Dict] = field(default_factory=list)
    
    # By claim type
    scores_by_type: Dict[str, float] = field(default_factory=dict)
    
    # Recommendations
    requires_human_review: bool = False
    review_reason: Optional[str] = None
    
    # Metadata
    generated_at: datetime = field(default_factory=datetime.now)
    source_count: int = 0
    
    def to_dict(self) -> Dict:
        return {
            "overall_score": self.overall_score,
            "total_claims": self.total_claims,
            "grounded_claims": self.grounded_claims,
            "ungrounded_claims": self.ungrounded_claims,
            "meets_threshold": self.meets_threshold,
            "requires_human_review": self.requires_human_review,
            "scores_by_type": self.scores_by_type,
            "claim_details": [s.to_dict() for s in self.claim_scores],
            "generated_at": self.generated_at.isoformat(),
        }
    
    def format_summary(self) -> str:
        """Format as human-readable summary."""
        status = "✅ PASS" if self.meets_threshold else "❌ FAIL"
        
        lines = [
            f"Grounding Report - {self.generated_at.strftime('%Y-%m-%d %H:%M')}",
            f"Overall: {status} ({self.overall_score:.0%})",
            f"Claims: {self.grounded_claims}/{self.total_claims} grounded",
            "",
        ]
        
        # By type breakdown
        if self.scores_by_type:
            lines.append("By Claim Type:")
            for claim_type, score in self.scores_by_type.items():
                lines.append(f"  - {claim_type}: {score:.0%}")
            lines.append("")
        
        # Ungrounded claims
        if self.ungrounded_details:
            lines.append("⚠️ Ungrounded Claims:")
            for detail in self.ungrounded_details[:5]:
                lines.append(f"  - {detail['claim_text'][:60]}...")
            lines.append("")
        
        if self.requires_human_review:
            lines.append(f"⚠️ Human Review Required: {self.review_reason}")
        
        return "\n".join(lines)


class GroundingReportGenerator:
    """
    Generates grounding reports for LLM outputs.
    
    Orchestrates claim extraction, source matching,
    and scoring into a comprehensive report.
    """
    
    def __init__(
        self,
        min_overall_score: float = 0.85,
        min_claim_confidence: float = 0.7,
        human_review_threshold: float = 0.70
    ):
        """
        Initialize report generator.
        
        Args:
            min_overall_score: Minimum % of claims that must be grounded
            min_claim_confidence: Minimum confidence per claim
            human_review_threshold: Below this, flag for human review
        """
        self.min_overall_score = min_overall_score
        self.min_claim_confidence = min_claim_confidence
        self.human_review_threshold = human_review_threshold
        
        self.extractor = ClaimExtractor()
        self.matcher = SourceMatcher()
        self.scorer = GroundingScorer(min_confidence=min_claim_confidence)
    
    def generate_report(
        self,
        llm_output: str,
        sources: List[Source]
    ) -> GroundingReport:
        """
        Generate grounding report for LLM output.
        
        Args:
            llm_output: Text generated by LLM
            sources: Available source documents
        
        Returns:
            GroundingReport
        """
        # Extract claims
        claims = self.extractor.extract_claims(llm_output)
        
        if not claims:
            logger.info("No claims to verify")
            return GroundingReport(
                overall_score=1.0,  # No claims = vacuously true
                total_claims=0,
                grounded_claims=0,
                ungrounded_claims=0,
                min_threshold=self.min_overall_score,
                meets_threshold=True,
                source_count=len(sources),
            )
        
        # Score each claim
        claim_scores = []
        
        for claim in claims:
            matches = self.matcher.find_matches(claim, sources)
            score = self.scorer.score_claim(claim, matches)
            claim_scores.append(score)
        
        # Aggregate metrics
        grounded_count = sum(1 for s in claim_scores if s.is_grounded)
        ungrounded_count = len(claims) - grounded_count
        
        overall_score = grounded_count / len(claims) if claims else 1.0
        meets_threshold = overall_score >= self.min_overall_score
        
        # Ungrounded details
        ungrounded_details = []
        for score in claim_scores:
            if not score.is_grounded:
                ungrounded_details.append({
                    "claim_text": score.claim.text,
                    "claim_type": score.claim.claim_type.value,
                    "confidence": score.overall_score,
                    "explanation": score.explanation,
                })
        
        # Scores by claim type
        scores_by_type = {}
        type_counts = {}
        
        for score in claim_scores:
            claim_type = score.claim.claim_type.value
            if claim_type not in scores_by_type:
                scores_by_type[claim_type] = 0
                type_counts[claim_type] = 0
            
            scores_by_type[claim_type] += score.overall_score
            type_counts[claim_type] += 1
        
        # Average by type
        for claim_type in scores_by_type:
            scores_by_type[claim_type] /= type_counts[claim_type]
        
        # Determine if human review needed
        requires_review = False
        review_reason = None
        
        if overall_score < self.human_review_threshold:
            requires_review = True
            review_reason = f"Overall grounding score ({overall_score:.0%}) below threshold"
        elif any(s.claim.claim_type == ClaimType.CAUSAL and not s.is_grounded for s in claim_scores):
            requires_review = True
            review_reason = "Ungrounded causal claims present"
        
        return GroundingReport(
            overall_score=overall_score,
            total_claims=len(claims),
            grounded_claims=grounded_count,
            ungrounded_claims=ungrounded_count,
            min_threshold=self.min_overall_score,
            meets_threshold=meets_threshold,
            claim_scores=claim_scores,
            ungrounded_details=ungrounded_details,
            scores_by_type=scores_by_type,
            requires_human_review=requires_review,
            review_reason=review_reason,
            source_count=len(sources),
        )


def generate_grounding_report(
    llm_output: str,
    sources: List[Source],
    min_score: float = 0.85
) -> GroundingReport:
    """
    Convenience function to generate grounding report.
    
    Args:
        llm_output: LLM-generated text
        sources: Source documents for verification
        min_score: Minimum acceptable grounding score
    
    Returns:
        GroundingReport
    """
    generator = GroundingReportGenerator(min_overall_score=min_score)
    return generator.generate_report(llm_output, sources)
